#!/usr/bin/env python3

# I M P O R T   M O D U L E S 
import argparse
import socket
import ssl
import re

# V A R I A B L E S
HTTP_VERSION = 'HTTP/1.1'
DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443


# C R A W L E R   C L A S S
class Crawler:
    # CONSTRUCTOR
    def __init__(self, cl_input):
        self.middle_token = None
        self.csrf_token = None
        self.session_id = None
        self.server = cl_input.server
        self.port = cl_input.port
        self.username = cl_input.username
        self.password = cl_input.password

    # METHOD: execute crawler class
    def run(self):
        # setup socket, wrap, and connect
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        context = ssl.create_default_context()
        sock.connect((self.server, self.port))
        sock = context.wrap_socket(sock, server_hostname=self.server)

        # create initial GET request and send
        sock.send(self.get_request().encode('ascii'))

        # recv response and print
        data = sock.recv(4000).decode('ascii')
        self.parse_response(data)

        # if self.csrf_token is None or self.middle_token is None or self.session_id is None:
        #     self.run()
        #     return

        # # create POST request and send
        # sock.send(self.post_request().encode('ascii'))

        # recv response and print
        # data = sock.recv(4000).decode('ascii')
        # self.parse_response(data)

        # # isolate status code
        # status = re.search(r'HTTP/1.1 (\d{1})', postdata).group(1)
        # print("status code= %s" % status)

        # delegate based on status code
        # match status:
        #   case '2':
        #     # ...
        #   case '3':
        #     # ...
        #   case '4':
        #     # ...
        #   case '5':
        #     # ...
        #   case _:
        #     # ...

        # then have to do the actual flag searching ...

        # close socket
        sock.close()

    def get_request(self):
        get = (f'GET /accounts/login/ {HTTP_VERSION}\r\n'
               f'Host: {self.server}\r\n'
               f'Connection: Keep-Alive\r\n\r\n')
        print("GET Request to %s:%d" % (self.server, self.port))
        # print(get)
        return get

    def parse_response(self, response):
        # search for session id
        self.session_id = re.search(r'sessionid=([^;]*)', response).group(1)
        print("session_id = %s" % self.session_id)

        # search for csrf token
        self.csrf_token = re.search(r'csrftoken=([^;]*)', response).group(1)
        print("csrf = %s" % self.csrf_token)

        # try to find csrf middleware token
        self.middle_token = re.search(r'name="csrfmiddlewaretoken" value="\s*([^\n\r]*)', response)
        if self.middle_token is not None:
            self.middle_token = self.middle_token[1][:-2]
            print(self.middle_token)

    def post_request(self):
        body = (f'username={self.username}&password={self.password}'
                f'&csrfmiddlewaretoken={self.middle_token}'
                f'&next=/fakebook/')
        post = (f'POST /accounts/login/ HTTP/1.1\r\n'
                f'Host: {self.server}\r\n'
                f'Cookie: sessionid={self.session_id}; csrftoken={self.csrf_token}\r\n'
                f'Accept-Encoding: gzip\r\n'
                f'Content-Type: application/x-www-form-urlencoded\r\n'
                f'Content-Length: {len(body)}\r\n'
                f'\r\n'
                f'{body}\r\n\r\n')

        return post


# M A I N   M E T H O D
if __name__ == "__main__":
    # setup arg parser
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()

    # setup crawler and execute
    sender = Crawler(args)
    sender.run()
