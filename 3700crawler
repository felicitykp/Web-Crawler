#!/usr/bin/env python3

# I M P O R T   M O D U L E S 
import argparse
import socket
import ssl
import re

# V A R I A B L E S
HTTP_VERSION = 'HTTP/1.1'
DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

# C R A W L E R   C L A S S
class Crawler:
    # CONSTRUCTOR
    def __init__(self, args):
        # sock vars
        self.sock = None
       	# command line vars
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
	# http header vars
        self.middle_token = None
        self.csrf_token = None
        self.session_id = None
        # functional vars
        self.flags = []
        self.explored = []
        self.unexplored = []

    # METHOD: craft GET request
    def get_request(self, addr):
        get = (f'GET /accounts/login{addr} {HTTP_VERSION}\r\n'
               f'Host: {self.server}\r\n'
               f'Connection: Keep-Alive\r\n\r\n')
        print("GET Request to %s:%d" % (self.server, self.port))
        # print(get)
        return get

    # METHOD: parse provided response for http header vars
    def parse_response(self, response):
        # search for session id
        self.session_id = re.search(r'sessionid=([^;]*)', response)
        if self.session_id is not None:
            self.session_id = self.session_id[1]
            print("session_id = %s" % self.session_id)

        # search for csrf token
        self.csrf_token = re.search(r'csrftoken=([^;]*)', response)
        if self.csrf_token is not None:
            self.csrf_token = self.csrf_token[1]
            print("csrf = %s" % self.csrf_token)

        # search for csrf middleware token
        self.middle_token = re.search(r'name="csrfmiddlewaretoken" value="\s*([^\n\r]*)', response)
        if self.middle_token is not None:
            self.middle_token = self.middle_token[1][:-2]
            print("middlewaretoken = %s" % self.middle_token)

    # METHOD: craft POST request
    def post_request(self, addr):
        body = (f'username={self.username}&password={self.password}'
                f'&csrfmiddlewaretoken={self.middle_token}'
                f'&next=/fakebook/')
        post = (f'POST /accounts/login{addr} {HTTP_VERSION}\r\n'
                f'Host: {self.server}\r\n'
                f'Cookie: sessionid={self.session_id}; csrftoken={self.csrf_token}\r\n'
                f'Accept-Encoding: gzip\r\n'
                f'Content-Type: application/x-www-form-urlencoded\r\n'
                f'Content-Length: {len(body)}\r\n'
                f'\r\n'
                f'{body}\r\n\r\n')
        print("POST Request to %s:%d" % (self.server, self.port))
        return post

    # METHOD: navigate to a new page
    def goto(self, addr):
        # print that we are redirecting
        print("REDIRECTING to %s" % addr)

        # send GET request and recv response
        self.sock.send(self.get_request(addr).encode('ascii'))
        get_data = self.sock.recv(4000).decode('ascii')

        # send POST request and recv response
        self.sock.send(self.post_request(addr).encode('ascii'))
        post_data = self.sock.recv(4000).decode('ascii')
        print("\nRESPONSE:\n")
        print(post_data)

        # isolate status code
        status = re.search(r'HTTP/1.1 (\d{1})', post_data).group(1)
        print("status code= %s" % status)

        # delegate based on status code
        if status == '2':
            # look for any flags on page
            for flag in re.findall(r'FLAG:\s*([^\n\r]*)', post_data):
                self.flags.append(flag)
        elif status == '3':
            # look for any links on page
            for link in re.findall(r'href=\s*([^\n\r]*)', post_data):
                # check we haven't been to page already
                if link in self.explored:
                    self.goto(link)
                    self.explored.append(link)
        elif status == '4':
            # abandon the url
            return
        elif status == '5':
            # retry goto
            self.goto(addr)
        else:
            print("ERROR: unrecognized error code. relaunching.")
            self.sock.close()
            self.run()
            return

    # METHOD: execute crawler class
    def run(self):
        # setup socket, wrap, and connect
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        context = ssl.create_default_context()
        self.sock.connect((self.server, self.port))
        self.sock = context.wrap_socket(self.sock, server_hostname=self.server)

        # send GET request and recv response
        self.sock.send(self.get_request('/').encode('ascii'))
        get_data = self.sock.recv(4000).decode('ascii')
        # print("\nRESPONSE:\n")
        # print(data)
        self.parse_response(get_data)

        # determine if necessary vars found
        if self.csrf_token is None or self.session_id is None or self.middle_token is None:
            print("ERROR: csrf_token and/or session_id and/or middle_token not found.")
            print("Reattempting initial GET request.\n")
            self.sock.close()
            self.run()
            return

        # create POST request and recv response
        self.sock.send(self.post_request('/').encode('ascii'))
        post_data = self.sock.recv(4000).decode('ascii')
        print("\nRESPONSE:\n")
        print(post_data)
        self.parse_response(post_data)
           
        # isolate redirect address and goto
        addr = re.search(r'Location:\s*([^\n\r]*)', post_data)[1]
        self.goto(addr)

        # close socket
        self.sock.close()

# M A I N   M E T H O D
if __name__ == "__main__":
    # setup arg parser
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()

    # setup crawler and execute
    sender = Crawler(args)
    sender.run()
